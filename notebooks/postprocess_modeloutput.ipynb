{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a48de523",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b5ecdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "\n",
    "project_root = Path.cwd().parents[0]\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from utils.bathymetry import generate_bathymetry\n",
    "from utils.config import load_config, default_params\n",
    "from utils.io import read_raw_output, ensure_dir\n",
    "from utils.grid import prepare_dsH, interp_ds, mean_onH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02898e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output folder for postprocessed data\n",
    "output_folder = str(project_root)+\"/output/processed/\"\n",
    "\n",
    "\n",
    "# focus depth contour will be in the middle of the slope\n",
    "focus_j = 45\n",
    "\n",
    "\n",
    "# subregion for analysis\n",
    "analysis_region = {\n",
    "    \"short\": {\n",
    "        \"focus_time_start\" : -(128+8)*8,\n",
    "        \"focus_time_end\"   : -8*8,\n",
    "        \"focus_j_start\": 20,\n",
    "        \"focus_j_end\": 70,\n",
    "    },\n",
    "    \"long\": {\n",
    "        \"focus_time_start\" : -(128+64)*8,\n",
    "        \"focus_time_end\"   : -64*8,\n",
    "        \"focus_j_start\": 20,\n",
    "        \"focus_j_end\": 70,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def calculate_analytical_estimates_xr(\n",
    "    ds: xr.Dataset,\n",
    "    forcing_vars: list[str],\n",
    "    params: dict,\n",
    ") -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    y(t_j) = ∫_0^{t_j} exp[-(R/H(j)) * (t_j - τ)] * F(τ, j) dτ\n",
    "    One-pass recurrence (left Riemann per step) with constant Δt = params[\"outputtime\"].\n",
    "    Recurrence: y_i = decay * y_{i-1} + alpha * F_{i-1},\n",
    "    where decay = exp(-k*Δt), alpha = (1 - decay)/k, k = R/H(j).\n",
    "    \"\"\"\n",
    "    R = float(params[\"R\"])\n",
    "    dt = float(params[\"outputtime\"]) \n",
    "\n",
    "    H = ds[\"depth\"]  \n",
    "    \n",
    "    # Sum forcing fields (must have dims include \"time\" and \"j\")\n",
    "    F = sum(ds[v] for v in forcing_vars)\n",
    "\n",
    "    t = ds[\"time\"]\n",
    "    nT = t.sizes[\"time\"]\n",
    "    if nT == 0:\n",
    "        raise ValueError(\"Empty time axis.\")\n",
    "    if nT == 1:\n",
    "        return xr.zeros_like(F)\n",
    "\n",
    "    # k(j) = R/H(j), broadcast over all non-time dims of F\n",
    "    k = (R / H).broadcast_like(F.isel(time=0))\n",
    "\n",
    "    # Precompute coefficients (constant in time)\n",
    "    decay = np.exp(-k * dt)\n",
    "    # Safe alpha for k≈0: limit -> dt\n",
    "    alpha = xr.where(np.abs(k) > 0, (1.0 - decay) / k, dt)\n",
    "\n",
    "    # build y \n",
    "    y0 = xr.zeros_like(F.isel(time=0, drop=True))\n",
    "    ys = [y0]\n",
    "    for i in range(1, nT):\n",
    "        Fi_1 = F.isel(time=i - 1, drop=True)  # drop time here too\n",
    "        y_next = decay * ys[-1] + alpha * Fi_1\n",
    "        ys.append(y_next)\n",
    "    y = xr.concat(ys, dim=\"time\").assign_coords(time=t) / H\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23251fb9",
   "metadata": {},
   "source": [
    "## Focus cases\n",
    "Process focus cases with uniform along-slope. There is one long-period case (128 days) and one short-period case (16 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad64a70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration from ../configs/baseline_forcing/long.json\n",
      "Directory created: /itf-fi-ml/home/alsjur/temporal-topo-flow/output/processed/timeseries\n",
      "Directory created: /itf-fi-ml/home/alsjur/temporal-topo-flow/output/processed/momentum_terms_H\n",
      "Directory created: /itf-fi-ml/home/alsjur/temporal-topo-flow/output/processed/momentum_terms_y\n",
      "Loading configuration from ../configs/baseline_forcing/short.json\n"
     ]
    }
   ],
   "source": [
    "for case in [\"long\", \"short\"]:\n",
    "    # load data\n",
    "    params = load_config(f\"../configs/baseline_forcing/{case}.json\")\n",
    "    ds = read_raw_output(params)\n",
    "    \n",
    "\n",
    "    #####################################################\n",
    "    #### DEPTH-FOLLOWING INTERPOLATION AND DIAGNOSES ####\n",
    "    #####################################################\n",
    "    \n",
    "    ### interpolate to depth-following grid ###\n",
    "    # determine target depths H_targets (mean depth along xC at each yC)\n",
    "    H_targets = ds.bath.mean(\"xC\").values\n",
    "    \n",
    "    dsH = prepare_dsH(ds, params, H_targets)\n",
    "        \n",
    "    ### calculate momentum terms ###\n",
    "    dsH[\"circulation\"] = mean_onH(dsH, variable=\"ui\")\n",
    "    dsH[\"BS\"] = -dsH.circulation*params[\"R\"]\n",
    "    #dsH[\"TFS\"] = dsH.circulation*0\n",
    "    dsH[\"RVF\"] = mean_onH(dsH, variable=\"zetaflux\") * dsH.depth\n",
    "    dsH[\"SS\"] = mean_onH(dsH, variable=\"forcing_i\") \n",
    "\n",
    "    ### timeseries at focus depth contour ###\n",
    "    \n",
    "    # circulation estimates\n",
    "    dsH[\"linear_estimate\"] = calculate_analytical_estimates_xr(dsH, [\"SS\"], params)\n",
    "    dsH[\"nonlinear_estimate\"] = calculate_analytical_estimates_xr(dsH, [\"SS\", \"RVF\"], params)\n",
    "    \n",
    "    ts = xr.Dataset()\n",
    "    ts[\"circulation\"] = dsH[\"circulation\"].isel(j=focus_j)\n",
    "    ts[\"linear_estimate\"] = dsH[\"linear_estimate\"].isel(j=focus_j)\n",
    "    ts[\"nonlinear_estimate\"] = dsH[\"nonlinear_estimate\"].isel(j=focus_j)\n",
    "    \n",
    "    # select focus period\n",
    "    ts = ts.isel(\n",
    "        time=slice(\n",
    "            analysis_region[case][\"focus_time_start\"],\n",
    "            analysis_region[case][\"focus_time_end\"],\n",
    "        ),\n",
    "    )\n",
    "    ts[\"time\"] = ts[\"time\"] - ts[\"time\"].values[0]\n",
    "    \n",
    "    ensure_dir(output_folder+\"/timeseries/\")\n",
    "    ts.squeeze().to_netcdf(output_folder+f\"/timeseries/analytical_estimates_{case}.nc\")\n",
    "    \n",
    "    \n",
    "    # select focus region and period\n",
    "    dsH = dsH.isel(\n",
    "        time=slice(\n",
    "            analysis_region[case][\"focus_time_start\"],\n",
    "            analysis_region[case][\"focus_time_end\"],\n",
    "        ),\n",
    "        j=slice(\n",
    "            analysis_region[case][\"focus_j_start\"],\n",
    "            analysis_region[case][\"focus_j_end\"],\n",
    "        ),\n",
    "    )\n",
    "    dsH[\"time\"] = dsH[\"time\"] - dsH[\"time\"].values[0]  # reset time to start at 0\n",
    "    \n",
    "    \n",
    "    ### save relevant momentum terms ###\n",
    "    if case == \"short\":\n",
    "        dsH = dsH.isel(time=slice(-16*8-1, None))  # only last 16 days for short case\n",
    "        dsH[\"time\"] = dsH[\"time\"] - dsH[\"time\"].values[0]  # reset time to start at 0\n",
    "        \n",
    "    ensure_dir(output_folder+\"/momentum_terms_H/\")\n",
    "    dsH[[\"circulation\", \"RVF\", \"BS\", \"SS\"]].to_netcdf(\n",
    "        output_folder+f\"/momentum_terms_H/momentum_terms_H_{case}.nc\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #####################################################\n",
    "    ############### CARTESIAN DIAGNOSES #################\n",
    "    #####################################################\n",
    "    \n",
    "    dsY = interp_ds(ds, params, [\"u\", \"v\", \"zetav\",\"forcing_x\", \"detadx\", \"duvhdy\"])\n",
    "    dsY = dsY.isel(\n",
    "        time=slice(\n",
    "            analysis_region[case][\"focus_time_start\"],\n",
    "            analysis_region[case][\"focus_time_end\"],\n",
    "        ),\n",
    "        yC=slice(\n",
    "            analysis_region[case][\"focus_j_start\"],\n",
    "            analysis_region[case][\"focus_j_end\"],\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    if case == \"short\":\n",
    "        dsY = dsY.isel(time=slice(-16*8, None))  # only last 16 days for short case   \n",
    "    \n",
    "    dsY[\"time\"] = dsY[\"time\"] - dsY[\"time\"].values[0]  # reset time to start at 0\n",
    "\n",
    "    # calculate momentum terms\n",
    "    H0 = dsY.bath.mean(\"xC\")\n",
    "    h = H0 - dsY.bath \n",
    "\n",
    "    dsY[\"circulation\"] = dsY.u.mean(\"xC\")\n",
    "    dsY[\"BS\"] = -dsY.circulation*params[\"R\"]\n",
    "    dsY[\"TFS\"] = (-params[\"gravitational_acceleration\"]*dsY.detadx*dsY.bath).mean(\"xC\")\n",
    "    dsY[\"MFC\"] = (-dsY.duvhdy).mean(\"xC\")\n",
    "    dsY[\"SS\"] = (dsY.forcing_x).mean(\"xC\")\n",
    "    dsY[\"QGPVF\"] = (dsY.zetav).mean(\"xC\")*H0 + (dsY.v*h).mean(\"xC\")*params[\"f\"]\n",
    "    \n",
    "    \n",
    "    # save relevant momentum terms\n",
    "    ensure_dir(output_folder+\"/momentum_terms_y/\")\n",
    "    dsY[[\"circulation\", \"BS\",\"TFS\", \"MFC\", \"SS\", \"QGPVF\"]].to_netcdf(\n",
    "        output_folder+f\"/momentum_terms_y/momentum_terms_y_{case}.nc\"\n",
    "    )   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f50798",
   "metadata": {},
   "source": [
    "## No-bumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c65c6458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration from ../configs/baseline_forcing/long_nobumps.json\n",
      "Loading configuration from ../configs/baseline_forcing/short_nobumps.json\n",
      "Loading configuration from ../configs/baseline_forcing/long_nobumps_crosswind.json\n",
      "Loading configuration from ../configs/baseline_forcing/short_nobumps_crosswind.json\n"
     ]
    }
   ],
   "source": [
    "for case in [\"long_nobumps\", \"short_nobumps\", \"long_nobumps_crosswind\", \"short_nobumps_crosswind\"]:\n",
    "    # load data\n",
    "    params = load_config(f\"../configs/baseline_forcing/{case}.json\")\n",
    "    ds = read_raw_output(params)\n",
    "    \n",
    "\n",
    "\n",
    "    dsY = interp_ds(ds, params, [\"u\", \"v\", \"zetav\",\"forcing_x\", \"detadx\", \"duvhdy\"])\n",
    "    dsY = dsY.isel(\n",
    "        time=slice(\n",
    "            analysis_region[case.split(\"_\")[0]][\"focus_time_start\"],\n",
    "            analysis_region[case.split(\"_\")[0]][\"focus_time_end\"],\n",
    "        ),\n",
    "        yC=slice(\n",
    "            analysis_region[case.split(\"_\")[0]][\"focus_j_start\"],\n",
    "            analysis_region[case.split(\"_\")[0]][\"focus_j_end\"],\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    if case.split(\"_\")[0] == \"short\":\n",
    "        dsY = dsY.isel(time=slice(-16*8, None))  # only last 16 days for short case   \n",
    "    \n",
    "    dsY[\"time\"] = dsY[\"time\"] - dsY[\"time\"].values[0]  # reset time to start at 0\n",
    "\n",
    "\n",
    "\n",
    "    dsY[\"circulation\"] = dsY.u.mean(\"xC\")\n",
    "    dsY[\"TFS\"] = (-params[\"gravitational_acceleration\"]*dsY.detadx*dsY.bath).mean(\"xC\")\n",
    "    dsY[\"MFC\"] = (-dsY.duvhdy).mean(\"xC\")\n",
    "    dsY[\"SS\"] = (dsY.forcing_x).mean(\"xC\")\n",
    "    dsY[\"BS\"] = -dsY.circulation*params[\"R\"]\n",
    "    \n",
    "    # time-mean terms\n",
    "    dsY[\"TFSy\"] = dsY[\"TFS\"].mean(\"time\")\n",
    "    dsY[\"MFCy\"] = dsY[\"MFC\"].mean(\"time\")\n",
    "    dsY[\"SSy\"] = dsY[\"SS\"].mean(\"time\")\n",
    "    dsY[\"BSy\"] = dsY[\"BS\"].mean(\"time\")\n",
    "    \n",
    "    # y meaned terms\n",
    "    dsY[\"TFSt\"] = dsY[\"TFS\"].mean(\"yC\")\n",
    "    dsY[\"MFCt\"] = dsY[\"MFC\"].mean(\"yC\")\n",
    "    dsY[\"SSt\"] = dsY[\"SS\"].mean(\"yC\")\n",
    "    dsY[\"BSt\"] = dsY[\"BS\"].mean(\"yC\")\n",
    "    \n",
    "    # save relevant momentum terms\n",
    "    ensure_dir(output_folder+\"/momentum_terms_y/\")\n",
    "    dsY[[\"circulation\", \"TFSt\", \"MFCt\", \"SSt\", \"BSt\",\"TFSy\", \"MFCy\", \"SSy\", \"BSy\"]].to_netcdf(\n",
    "        output_folder+f\"/momentum_terms_y/momentum_terms_y_{case}.nc\"\n",
    "    )   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c31ddf",
   "metadata": {},
   "source": [
    "# Wave calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a2bdf",
   "metadata": {},
   "source": [
    "## Arrest speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bee92314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_prograde_retrograde_speed(dsH,selection=slice(40,50), spread=True):\n",
    "    slope = dsH.sel(j=selection).circulation\n",
    "    #slope = dsH.sel(j=slice(40,50)).circulation    \n",
    "    \n",
    "    if spread:\n",
    "        prograde = slope.max(\"time\").mean(\"j\").item()\n",
    "        retrograde = np.abs(slope.min(\"time\").mean(\"j\").item())\n",
    "        \n",
    "        retrograde_max = np.abs(slope.min(\"time\").min(\"j\").item())\n",
    "        retrograde_min = np.abs(slope.min(\"time\").max(\"j\").item())\n",
    "    \n",
    "    \n",
    "        return retrograde, prograde, retrograde_min, retrograde_max\n",
    "\n",
    "    else:\n",
    "        retrograde = np.abs(np.min(slope))\n",
    "        prograde = np.max(slope)\n",
    "        \n",
    "        return retrograde, prograde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7d14e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration from ../configs/varying_bathymetry/half.json\n",
      "Loading configuration from ../configs/baseline_forcing/long.json\n",
      "Loading configuration from ../configs/varying_bathymetry/double.json\n",
      "Directory created: /itf-fi-ml/home/alsjur/temporal-topo-flow/output/processed/wave_comparison\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params_22km = load_config(\"../configs/varying_bathymetry/half.json\")\n",
    "params_45km = load_config(\"../configs/baseline_forcing/long.json\")\n",
    "params_90km = load_config(\"../configs/varying_bathymetry/double.json\")\n",
    "\n",
    "arrest_speed_results = {}\n",
    "\n",
    "for params, wavelength in zip([params_22km, params_45km, params_90km], [22.5, 45, 90]):\n",
    "    ds = read_raw_output(params)\n",
    "    dsH = prepare_dsH(ds, params, H_targets)\n",
    "    \n",
    "    # diagnose circulation\n",
    "    dsH[\"circulation\"] = mean_onH(dsH, variable=\"ui\")\n",
    "    \n",
    "    retrograde, prograde, retrograde_min, retrograde_max = estimate_prograde_retrograde_speed(dsH)\n",
    "    \n",
    "    arrest_speed_results[wavelength] = {\n",
    "        \"retrograde\": retrograde,\n",
    "        \"prograde\": prograde,\n",
    "        \"retrograde_min\": retrograde_min,\n",
    "        \"retrograde_max\": retrograde_max,\n",
    "    }\n",
    "    \n",
    "ensure_dir(output_folder+\"/wave_comparison/\")\n",
    "json.dump(arrest_speed_results, open(output_folder+\"/wave_comparison/arrest_speeds.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f07df",
   "metadata": {},
   "source": [
    "## Mode structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e934ce46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration from ../configs/baseline_forcing/long.json\n"
     ]
    }
   ],
   "source": [
    "params = load_config(\"../configs/baseline_forcing/long.json\")\n",
    "ds = read_raw_output(params)\n",
    "ds = ds.isel(\n",
    "    time=slice(\n",
    "        analysis_region[\"long\"][\"focus_time_start\"],\n",
    "        analysis_region[\"long\"][\"focus_time_end\"],\n",
    "    ),\n",
    ")\n",
    "ds[\"time\"] = ds[\"time\"] - ds[\"time\"].values[0]  # reset time to start at 0\n",
    "\n",
    "eta = ds[\"h\"] - ds[\"bath\"]\n",
    "etanod = eta - eta.mean(dim=\"xC\")\n",
    "\n",
    "etanod_neg = etanod.isel(time=slice(0,64*8))\n",
    "etanod_pos = etanod.isel(time=slice(64*8,None))\n",
    "etanod_pos[\"time\"] = etanod_pos[\"time\"] - etanod_pos[\"time\"].isel(time=0)\n",
    "\n",
    "mode = etanod_neg + etanod_pos \n",
    "\n",
    "mode.to_netcdf(output_folder+\"/wave_comparison/mode_structure.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb1bc5e",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae27e90",
   "metadata": {},
   "source": [
    "## Max speed for varying forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cda02579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration from ../configs/varying_forcing/short_001.json\n",
      "Loading configuration from ../configs/varying_forcing/short_002.json\n",
      "Loading configuration from ../configs/varying_forcing/short_003.json\n",
      "Loading configuration from ../configs/varying_forcing/short_004.json\n",
      "Loading configuration from ../configs/varying_forcing/short_005.json\n",
      "Loading configuration from ../configs/varying_forcing/short_006.json\n",
      "Loading configuration from ../configs/varying_forcing/short_007.json\n",
      "Loading configuration from ../configs/varying_forcing/short_008.json\n",
      "Loading configuration from ../configs/varying_forcing/long_001.json\n",
      "Loading configuration from ../configs/varying_forcing/long_002.json\n",
      "Loading configuration from ../configs/varying_forcing/long_003.json\n",
      "Loading configuration from ../configs/varying_forcing/long_004.json\n",
      "Loading configuration from ../configs/varying_forcing/long_005.json\n",
      "Loading configuration from ../configs/varying_forcing/long_006.json\n",
      "Loading configuration from ../configs/varying_forcing/long_007.json\n",
      "Loading configuration from ../configs/varying_forcing/long_008.json\n"
     ]
    }
   ],
   "source": [
    "depths_to_use = [H_targets[40], H_targets[45], H_targets[50]]\n",
    "periods = [\"short\", \"long\"]\n",
    "n_runs = 8 \n",
    "\n",
    "# --- Pre-allocate tidy dataset ---\n",
    "ds_out = xr.Dataset(\n",
    "    coords=dict(\n",
    "        period=(\"period\", periods),\n",
    "        run=(\"run\", np.arange(1, n_runs + 1)),\n",
    "        depth=(\"depth\", depths_to_use),\n",
    "    ),\n",
    "    data_vars=dict(\n",
    "        forcing_strength=((\"period\", \"run\"), np.full((len(periods), n_runs), np.nan)),\n",
    "        prograde_max=((\"period\", \"depth\", \"run\"), np.full((len(periods), len(depths_to_use), n_runs), np.nan)),\n",
    "        retrograde_max=((\"period\", \"depth\", \"run\"), np.full((len(periods), len(depths_to_use), n_runs), np.nan)),\n",
    "    ),\n",
    ")\n",
    "\n",
    "for p_idx, p in enumerate(periods):\n",
    "    for r in range(1, n_runs + 1):\n",
    "        params_i = load_config(f\"../configs/varying_forcing/{p}_{r:03d}.json\")\n",
    "        forcing = params_i[\"tau0\"]\n",
    "        ds_i = read_raw_output(params_i).isel(time=slice(-128*8, None))\n",
    "\n",
    "        dsH_i = prepare_dsH(ds_i, params_i, depths_to_use)\n",
    "        circ = mean_onH(dsH_i, variable=\"ui\")\n",
    "        dsH_i[\"circulation\"] = circ#hanning_filter(circ, window_length=2*8)\n",
    "\n",
    "        ds_out[\"forcing_strength\"][p_idx, r - 1] = forcing\n",
    "\n",
    "        for k, H in enumerate(dsH_i[\"depth\"].values):\n",
    "            retro, pro = estimate_prograde_retrograde_speed(dsH_i, selection=k, spread=False)\n",
    "            ds_out[\"prograde_max\"][p_idx, k, r - 1] = pro\n",
    "            ds_out[\"retrograde_max\"][p_idx, k, r - 1] = retro\n",
    "            \n",
    "ds_sorted = ds_out.copy() \n",
    "for p in periods: \n",
    "    F = ds_out[\"forcing_strength\"].sel(period=p) \n",
    "    order = np.argsort(F.values) \n",
    "    ds_sorted[\"forcing_strength\"].loc[dict(period=p)] = F.values[order] \n",
    "    for var in [\"prograde_max\", \"retrograde_max\"]: \n",
    "        ds_sorted[var].loc[dict(period=p)] = ds_out[var].sel(period=p).values[..., order]\n",
    "        \n",
    "vars_to_save = [\"forcing_strength\", \"prograde_max\", \"retrograde_max\"]\n",
    "ds_to_write = ds_sorted[vars_to_save]\n",
    "\n",
    "ds_to_write.to_netcdf(output_folder+\"/wave_comparison/forcing_vs_arrest_speeds.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba9841f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
